{
  "enabled": true,
  "name": "Database Migration Orchestrator",
  "description": "Automatically generates step-by-step migration prompts when database schema files change for safe AI implementation",
  "version": "1",
  "when": {
    "type": "fileEdited",
    "patterns": [
      "database/migrations/*.sql",
      "supabase/migrations/*.sql",
      "database/schema.sql",
      "supabase/schema.sql",
      "lib/types/database.ts",
      "types/supabase.ts"
    ]
  },
  "then": {
    "type": "askAgent",
    "prompt": "Database schema changes have been detected. Please analyze the changes and generate comprehensive, step-by-step migration prompts for safe AI implementation.\n\n**Critical Safety Notice:** Database migrations can cause data loss and application downtime. All migration tasks must be executed with extreme caution and proper backup procedures.\n\n## 1. Schema Change Analysis\n\n**Analyze Modified Files:**\n- **Migration Files**: Identify new or modified .sql migration files\n- **Schema Files**: Detect structural changes to database schema\n- **Type Files**: Check for TypeScript definition updates\n- **Related Services**: Find code that may be affected by schema changes\n\n**Change Classification:**\n- **Safe Changes**: Adding columns, indexes, new tables\n- **Risky Changes**: Dropping columns, changing data types, constraints\n- **Breaking Changes**: Removing tables, major structural modifications\n- **Data Migrations**: Changes requiring data transformation\n\n## 2. Generate Migration Safety Checklist\n\n**Create/Update: `.kiro/migration-safety-checklist.md`**\n\n```markdown\n# Database Migration Safety Checklist\n\n*Auto-generated: [timestamp]*\n*Migration Files: [list of changed files]*\n*Risk Level: [LOW/MEDIUM/HIGH/CRITICAL]*\n\n## 🚨 Pre-Migration Safety Requirements\n\n### Backup Verification\n- [ ] **CRITICAL**: Full database backup completed and verified\n- [ ] Backup restoration tested in isolated environment\n- [ ] Point-in-time recovery capability confirmed\n- [ ] Backup storage location documented and accessible\n\n**Backup Commands:**\n```bash\n# Supabase backup (if using Supabase CLI)\nsupabase db dump --file backup_$(date +%Y%m%d_%H%M%S).sql\n\n# PostgreSQL backup (direct connection)\npg_dump -h [host] -U [user] -d [database] > backup_$(date +%Y%m%d_%H%M%S).sql\n\n# Verify backup integrity\npsql -h [host] -U [user] -d test_restore < backup_file.sql\n```\n\n### Environment Preparation\n- [ ] Staging environment matches production schema\n- [ ] Migration tested successfully in staging\n- [ ] Application code compatible with both old and new schema\n- [ ] Rollback procedures documented and tested\n\n### Team Coordination\n- [ ] Migration scheduled during low-traffic period\n- [ ] Team members notified of maintenance window\n- [ ] Monitoring and alerting systems prepared\n- [ ] Emergency contacts and escalation procedures ready\n\n## 📊 Migration Impact Analysis\n\n### Schema Changes Detected\n\n#### [CHANGE_TYPE_1]: [Description]\n**Risk Level**: [LOW/MEDIUM/HIGH]\n**Affected Tables**: [table_names]\n**Estimated Duration**: [time_estimate]\n**Rollback Complexity**: [SIMPLE/MODERATE/COMPLEX]\n\n**Impact Assessment:**\n- **Data Loss Risk**: [YES/NO] - [Explanation]\n- **Downtime Required**: [YES/NO] - [Duration estimate]\n- **Application Compatibility**: [BACKWARD_COMPATIBLE/BREAKING]\n- **Performance Impact**: [Description of query performance changes]\n\n**Dependencies:**\n- [ ] Dependent table: [table_name] - [relationship]\n- [ ] Application code: [file_paths] - [required changes]\n- [ ] External integrations: [services] - [impact]\n\n#### [CHANGE_TYPE_2]: [Description]\n[Repeat for each detected change]\n\n### Data Migration Requirements\n\n**Data Transformation Needed:**\n- [ ] Column data type conversion\n- [ ] Data format standardization\n- [ ] Reference integrity updates\n- [ ] Default value population\n\n**Estimated Data Volume:**\n- **Affected Rows**: [approximate count]\n- **Processing Time**: [estimate based on data volume]\n- **Memory Requirements**: [for large data operations]\n\n## 🛠️ Step-by-Step Migration Implementation\n\n### Phase 1: Pre-Migration Preparation\n\n#### Task 1.1: Environment Validation\n**AI Prompt:**\n```\nValidate the database environment before migration:\n\n1. **Connection Testing**:\n   - Test database connectivity to all environments\n   - Verify user permissions for migration operations\n   - Check available disk space and memory\n   - Validate backup and restore capabilities\n\n2. **Schema Validation**:\n   - Compare current schema with expected pre-migration state\n   - Identify any unexpected schema drift\n   - Verify all dependent objects exist\n   - Check for blocking transactions or locks\n\n3. **Application Health Check**:\n   - Verify application is running normally\n   - Check error rates and performance metrics\n   - Ensure no ongoing data operations that could conflict\n   - Validate monitoring and alerting systems\n\n**Deliverables:**\n- Environment validation report\n- Pre-migration health check results\n- Go/no-go recommendation with reasoning\n\n**Success Criteria:**\n- All database connections successful\n- No schema drift detected\n- Application running within normal parameters\n- Backup systems operational\n```\n\n#### Task 1.2: Backup Creation and Verification\n**AI Prompt:**\n```\nCreate and verify database backups before migration:\n\n1. **Full Database Backup**:\n   - Create complete database dump with timestamp\n   - Include schema, data, and database objects\n   - Verify backup file integrity and completeness\n   - Document backup location and access procedures\n\n2. **Incremental Backup** (if applicable):\n   - Create point-in-time recovery checkpoint\n   - Document transaction log position\n   - Verify incremental backup chain integrity\n\n3. **Backup Testing**:\n   - Restore backup to isolated test environment\n   - Verify data integrity and completeness\n   - Test application connectivity to restored database\n   - Document restoration time and procedures\n\n**Commands to Execute:**\n```bash\n# Create timestamped backup\nBACKUP_FILE=\"cybernex_backup_$(date +%Y%m%d_%H%M%S).sql\"\nsupabase db dump --file $BACKUP_FILE\n\n# Verify backup size and content\nls -lh $BACKUP_FILE\nhead -n 50 $BACKUP_FILE\n\n# Test restoration (in isolated environment)\nsupabase db reset --file $BACKUP_FILE\n```\n\n**Success Criteria:**\n- Backup file created successfully\n- Backup restoration tested and verified\n- Backup procedures documented\n- Recovery time objectives met\n```\n\n### Phase 2: Migration Execution\n\n#### Task 2.1: Schema Migration Implementation\n**AI Prompt:**\n```\nExecute database schema migration with safety monitoring:\n\n1. **Pre-Migration Checks**:\n   - Verify backup completion and integrity\n   - Check database locks and active connections\n   - Confirm maintenance window and team readiness\n   - Enable detailed logging and monitoring\n\n2. **Migration Execution**:\n   - Execute migration scripts in correct order\n   - Monitor execution progress and performance\n   - Watch for errors, warnings, or unexpected behavior\n   - Verify each step completes successfully before proceeding\n\n3. **Progress Monitoring**:\n   - Track migration execution time\n   - Monitor database performance metrics\n   - Check for blocking operations or deadlocks\n   - Verify data integrity during migration\n\n**Migration Script Execution:**\n```sql\n-- Enable detailed logging\nSET log_statement = 'all';\nSET log_min_duration_statement = 0;\n\n-- Begin transaction for rollback capability\nBEGIN;\n\n-- Execute migration steps with verification\n[MIGRATION_SQL_CONTENT]\n\n-- Verify migration success\nSELECT 'Migration verification queries here';\n\n-- Commit only if all verifications pass\nCOMMIT;\n```\n\n**Safety Measures:**\n- Execute in transaction for rollback capability\n- Implement timeouts for long-running operations\n- Monitor system resources during execution\n- Maintain communication with team during migration\n\n**Success Criteria:**\n- All migration scripts execute without errors\n- Schema changes applied correctly\n- Data integrity maintained\n- Performance within acceptable limits\n```\n\n#### Task 2.2: Data Migration and Transformation\n**AI Prompt:**\n```\nPerform data migration and transformation safely:\n\n1. **Data Validation Pre-Migration**:\n   - Count records in affected tables\n   - Identify data quality issues\n   - Validate referential integrity\n   - Document baseline metrics\n\n2. **Data Transformation Execution**:\n   - Process data in manageable batches\n   - Implement progress tracking and logging\n   - Handle errors gracefully with rollback capability\n   - Verify data consistency after each batch\n\n3. **Data Integrity Verification**:\n   - Compare record counts before and after\n   - Validate transformed data meets new constraints\n   - Check referential integrity across tables\n   - Verify business logic constraints\n\n**Batch Processing Template:**\n```sql\n-- Process data in batches to avoid locks\nDO $$\nDECLARE\n    batch_size INTEGER := 1000;\n    processed INTEGER := 0;\n    total_records INTEGER;\nBEGIN\n    SELECT COUNT(*) INTO total_records FROM source_table;\n    \n    WHILE processed < total_records LOOP\n        -- Transform data batch\n        UPDATE target_table \n        SET new_column = transform_function(old_column)\n        WHERE id IN (\n            SELECT id FROM source_table \n            ORDER BY id \n            LIMIT batch_size OFFSET processed\n        );\n        \n        processed := processed + batch_size;\n        \n        -- Log progress\n        RAISE NOTICE 'Processed % of % records', processed, total_records;\n        \n        -- Commit batch to avoid long transactions\n        COMMIT;\n    END LOOP;\nEND $$;\n```\n\n**Success Criteria:**\n- All data transformed successfully\n- No data loss or corruption\n- Referential integrity maintained\n- Performance acceptable throughout process\n```\n\n### Phase 3: Post-Migration Validation\n\n#### Task 3.1: Schema and Data Verification\n**AI Prompt:**\n```\nVerify migration success and data integrity:\n\n1. **Schema Verification**:\n   - Compare actual schema with expected post-migration state\n   - Verify all tables, columns, and constraints created correctly\n   - Check indexes and performance optimization objects\n   - Validate permissions and security settings\n\n2. **Data Integrity Checks**:\n   - Run comprehensive data validation queries\n   - Verify record counts and data consistency\n   - Check referential integrity across all tables\n   - Validate business rules and constraints\n\n3. **Performance Validation**:\n   - Test query performance on modified tables\n   - Verify indexes are being used correctly\n   - Check for any performance regressions\n   - Monitor resource usage and optimization opportunities\n\n**Validation Queries:**\n```sql\n-- Schema validation\nSELECT table_name, column_name, data_type, is_nullable\nFROM information_schema.columns\nWHERE table_schema = 'public'\nORDER BY table_name, ordinal_position;\n\n-- Data integrity checks\nSELECT \n    table_name,\n    COUNT(*) as record_count,\n    COUNT(DISTINCT primary_key) as unique_records\nFROM affected_tables\nGROUP BY table_name;\n\n-- Referential integrity validation\nSELECT constraint_name, table_name, constraint_type\nFROM information_schema.table_constraints\nWHERE constraint_type = 'FOREIGN KEY';\n```\n\n**Success Criteria:**\n- Schema matches expected post-migration state\n- All data integrity checks pass\n- No performance regressions detected\n- All constraints and relationships functional\n```\n\n#### Task 3.2: Application Integration Testing\n**AI Prompt:**\n```\nTest application integration with migrated database:\n\n1. **Application Startup Testing**:\n   - Verify application starts successfully with new schema\n   - Check all database connections and pools\n   - Validate ORM/query builder compatibility\n   - Test authentication and authorization systems\n\n2. **Functionality Testing**:\n   - Test all CRUD operations on modified tables\n   - Verify API endpoints work with new schema\n   - Check user workflows and business processes\n   - Test error handling and edge cases\n\n3. **Performance Testing**:\n   - Monitor application response times\n   - Check database query performance\n   - Verify caching systems work correctly\n   - Test under normal and peak load conditions\n\n**Testing Checklist:**\n- [ ] User authentication and registration\n- [ ] Resource management (CRUD operations)\n- [ ] Learning path and progress tracking\n- [ ] Community features and interactions\n- [ ] Admin dashboard and management tools\n- [ ] API endpoints and external integrations\n- [ ] Search and filtering functionality\n- [ ] Payment and subscription systems\n\n**Automated Testing:**\n```bash\n# Run application test suite\nnpm run test\nnpm run test:integration\nnpm run test:e2e\n\n# Performance testing\nnpm run test:performance\n\n# API testing\nnpm run test:api\n```\n\n**Success Criteria:**\n- All application tests pass\n- No functionality regressions\n- Performance within acceptable limits\n- User workflows complete successfully\n```\n\n### Phase 4: Monitoring and Rollback Preparation\n\n#### Task 4.1: Post-Migration Monitoring Setup\n**AI Prompt:**\n```\nImplement comprehensive monitoring for post-migration stability:\n\n1. **Database Monitoring**:\n   - Set up alerts for query performance degradation\n   - Monitor connection pool usage and errors\n   - Track database resource utilization\n   - Alert on constraint violations or data errors\n\n2. **Application Monitoring**:\n   - Monitor error rates and response times\n   - Track user experience metrics\n   - Set up alerts for critical functionality failures\n   - Monitor business metrics and KPIs\n\n3. **Custom Migration Monitoring**:\n   - Create specific alerts for migration-related issues\n   - Monitor data consistency and integrity\n   - Track performance of modified queries\n   - Set up automated health checks\n\n**Monitoring Queries:**\n```sql\n-- Query performance monitoring\nSELECT query, mean_time, calls, total_time\nFROM pg_stat_statements\nWHERE query LIKE '%modified_table%'\nORDER BY mean_time DESC;\n\n-- Data consistency checks\nSELECT \n    COUNT(*) as total_records,\n    COUNT(CASE WHEN validation_column IS NULL THEN 1 END) as null_values,\n    COUNT(CASE WHEN validation_failed THEN 1 END) as validation_errors\nFROM migrated_table;\n```\n\n**Alert Configuration:**\n- Database error rate > 1%\n- Query response time > 2x baseline\n- Connection pool exhaustion\n- Data integrity constraint violations\n\n**Success Criteria:**\n- All monitoring systems operational\n- Baseline metrics established\n- Alert thresholds configured appropriately\n- Team notified of monitoring procedures\n```\n\n#### Task 4.2: Rollback Procedure Documentation\n**AI Prompt:**\n```\nDocument and test rollback procedures for emergency use:\n\n1. **Rollback Decision Criteria**:\n   - Define conditions that trigger rollback\n   - Establish decision-making authority\n   - Document escalation procedures\n   - Set maximum time limits for rollback decision\n\n2. **Rollback Procedure Documentation**:\n   - Step-by-step rollback instructions\n   - Required permissions and access\n   - Estimated rollback time and impact\n   - Communication procedures during rollback\n\n3. **Rollback Testing** (in staging):\n   - Test rollback procedure in staging environment\n   - Verify data restoration accuracy\n   - Check application compatibility with rolled-back schema\n   - Document any issues or complications\n\n**Rollback Triggers:**\n- Critical application errors > 5%\n- Database performance degradation > 50%\n- Data corruption or integrity issues\n- User-facing functionality failures\n- Security vulnerabilities introduced\n\n**Rollback Procedure:**\n```bash\n# 1. Stop application traffic (if possible)\n# 2. Create current state backup\nsupabase db dump --file pre_rollback_backup.sql\n\n# 3. Restore from pre-migration backup\nsupabase db reset --file $BACKUP_FILE\n\n# 4. Verify restoration\n# 5. Restart application\n# 6. Verify functionality\n# 7. Communicate status to team\n```\n\n**Success Criteria:**\n- Rollback procedures documented and tested\n- Decision criteria clearly defined\n- Team trained on rollback procedures\n- Emergency contacts and escalation ready\n```\n\n## 3. Risk-Based Migration Strategies\n\n### Low Risk Migrations\n**Characteristics:**\n- Adding new tables or columns\n- Creating indexes\n- Adding non-breaking constraints\n\n**Simplified Process:**\n- Standard backup and testing\n- Can be executed during normal hours\n- Minimal monitoring required\n\n### Medium Risk Migrations\n**Characteristics:**\n- Modifying existing columns\n- Adding foreign key constraints\n- Data type changes (compatible)\n\n**Enhanced Process:**\n- Extended testing period\n- Staged rollout if possible\n- Enhanced monitoring\n- Team standby during execution\n\n### High Risk Migrations\n**Characteristics:**\n- Dropping columns or tables\n- Breaking data type changes\n- Complex data transformations\n\n**Maximum Safety Process:**\n- Extended maintenance window\n- Full team availability\n- Comprehensive rollback testing\n- Staged execution with checkpoints\n\n### Critical Risk Migrations\n**Characteristics:**\n- Major structural changes\n- Large data migrations\n- Breaking application compatibility\n\n**Executive Approval Process:**\n- Business stakeholder approval\n- Extended planning period\n- Disaster recovery team standby\n- Customer communication plan\n\n## 4. Cybernex Academy Specific Considerations\n\n### Learning Data Protection\n- User progress and achievements\n- Course completion records\n- Learning path customizations\n- Assessment and certification data\n\n### Community Data Integrity\n- User profiles and preferences\n- Forum posts and discussions\n- Social connections and networking\n- User-generated content\n\n### Security and Compliance\n- User authentication and authorization\n- Payment and subscription data\n- Personal information protection\n- Audit trail maintenance\n\n### Business Continuity\n- Subscription service availability\n- Learning platform accessibility\n- Community feature functionality\n- Administrative capabilities\n\n## Instructions:\n1. **Analyze schema changes** and classify risk level\n2. **Generate appropriate migration strategy** based on risk assessment\n3. **Create detailed step-by-step prompts** for AI execution\n4. **Include comprehensive safety measures** and rollback procedures\n5. **Provide monitoring and validation steps** for post-migration\n6. **Document emergency procedures** and team coordination\n7. **Consider Cybernex Academy specific requirements** for user data and business continuity\n\nThe goal is to ensure that database migrations are executed safely with minimal risk to data integrity, application functionality, and user experience."
  }
}